{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from ardal import Ardal\n",
    "import _ardal\n",
    "import random\n",
    "\n",
    "\n",
    "def randcoords(npy_matrix, ncoords=10):\n",
    "    # npy_matrix = np.load(\"./data/test_csv_matrix.npy\")\n",
    "    # test_matrix = np.array([[0, 1, 1, 0],[1, 0, 0, 1], [0, 1, 0, 1], [1, 0, 0, 1]])\n",
    "\n",
    "    # coords = np.array([[0, 0], [0, 1], [0, 2], [0, 3], [2, 1]], dtype=int)\n",
    "\n",
    "    n_rows, n_cols = len(npy_matrix)-1, len(npy_matrix[0])-1\n",
    "    coords = []\n",
    "\n",
    "    # print(npy_matrix, n_rows, n_cols)\n",
    "\n",
    "\n",
    "    for _ in range(ncoords):\n",
    "        col = random.randint(0, n_cols - 1)\n",
    "        row = random.randint(0, n_rows - 1)\n",
    "        coords.append([row, col])\n",
    "\n",
    "    return np.array(coords, dtype=int)\n",
    "\n",
    "\n",
    "def getCoords(json_headers, allele_ids=None, guids=None):\n",
    "\n",
    "    if allele_ids == None and guids == None:\n",
    "        print(\"Please provide a list of guids/allele ids\")\n",
    "      \n",
    "    if guids == None:\n",
    "        guids = json_headers[\"guids\"]\n",
    "    \n",
    "    if allele_ids == None:\n",
    "        allele_ids = json_headers[\"alleles\"]\n",
    "        \n",
    "    coords = []\n",
    "    for aid in allele_ids:\n",
    "\n",
    "        if aid not in json_headers[\"alleles\"]:\n",
    "            raise ValueError(f\"Allele ID '{aid}' not found.\")\n",
    "\n",
    "        for guid in guids:\n",
    "            if guid not in json_headers[\"guids\"]:\n",
    "                raise ValueError(f\"Sample GUID '{guid}' not found.\")\n",
    "        \n",
    "            coords.append(encodeCoord(json_headers, [guid, aid]))\n",
    "\n",
    "    return coords\n",
    "\n",
    "\n",
    "def encodeCoord(json_headers, coord):\n",
    "    return [json_headers[\"guids\"].index(coord[0]), json_headers[\"alleles\"].index(coord[1])]\n",
    "\n",
    "def decodeCoord(json_headers, coord):\n",
    "    return [ json_headers[\"guids\"][coord[0]], json_headers[\"alleles\"][coord[1]]]\n",
    "\n",
    "def encodeGuid(json_headers, guid):\n",
    "    return json_headers[\"guids\"].index(guid)\n",
    "\n",
    "def decodeGuid(json_headers, row_coord):\n",
    "    return json_headers[\"guids\"][row_coord]\n",
    "\n",
    "def encodeAllele(json_headers, allele):\n",
    "    return json_headers[\"alleles\"].index(allele)\n",
    "\n",
    "def decodeAllele(json_headers, col_coord):\n",
    "    return json_headers[\"alleles\"][col_coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMatrix(sample_n=1000, allele_n=50000):\n",
    "\n",
    "    ## 1. Create the Matrix (random 0s and 1s)\n",
    "    npy_matrix = np.ascontiguousarray(np.random.randint(0, 2, size=(sample_n, allele_n), dtype='uint8'))  # Adjust as needed for your desired distribution\n",
    "\n",
    "    ## 2. GUIDs (Row Labels) - Simple numerical IDs\n",
    "    guids = [f\"sample_{i}\" for i in np.arange(sample_n)]\n",
    "\n",
    "    ## 3. Alleles (Column Labels) - Simple numerical IDs\n",
    "    alleles = [f\"allele_{i}\" for i in np.arange(allele_n)]\n",
    "\n",
    "    headers_json = {\"guids\" : guids, \"alleles\" : alleles}\n",
    "\n",
    "    return npy_matrix, headers_json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generateSparseMatrix(sample_n=1000, allele_n=50000, sparsity=0.05):\n",
    "    \"\"\"Generates a sparse binary matrix with a specified sparsity.\n",
    "\n",
    "    Args:\n",
    "        sample_n: Number of rows (samples).\n",
    "        allele_n: Number of columns (alleles).\n",
    "        sparsity: Target proportion of 1s in the matrix (e.g., 0.05 for 5%).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - npy_matrix: A NumPy array representing the sparse matrix.\n",
    "            - headers_json: A dictionary containing row and column labels.\n",
    "    \"\"\"\n",
    "\n",
    "    npy_matrix = np.zeros((sample_n, allele_n), dtype='uint8')\n",
    "\n",
    "    for i in range(sample_n):\n",
    "        # Calculate the number of 1s for the current row based on sparsity\n",
    "        num_ones = int(sparsity * allele_n)  # Or round if needed\n",
    "\n",
    "        # Generate random indices for the 1s\n",
    "        indices = np.random.choice(allele_n, size=num_ones, replace=False)\n",
    "\n",
    "        # Set the chosen indices to 1\n",
    "        npy_matrix[i, indices] = 1\n",
    "\n",
    "\n",
    "    guids = [f\"sample_{i}\" for i in np.arange(sample_n)]\n",
    "    alleles = [f\"allele_{i}\" for i in np.arange(allele_n)]\n",
    "\n",
    "    headers_json = {\"guids\": guids, \"alleles\": alleles}\n",
    "\n",
    "    return npy_matrix, headers_json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generateClusteredSparseMatrix(sample_n=1000, allele_n=50000, sparsity=0.05, n_clusters=10, cluster_spread=0.1):\n",
    "    \"\"\"Generates a sparse binary matrix with clustered data based on Hamming distance.\n",
    "\n",
    "    Args:\n",
    "        sample_n: Number of rows (samples).\n",
    "        allele_n: Number of columns (alleles).\n",
    "        sparsity: Target proportion of 1s in the matrix.\n",
    "        n_clusters: Number of clusters to generate.\n",
    "        cluster_spread: Controls the Hamming distance spread within clusters (higher values lead to more spread).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - npy_matrix: NumPy array representing the sparse matrix.\n",
    "            - headers_json: Dictionary containing row and column labels.\n",
    "    \"\"\"\n",
    "\n",
    "    npy_matrix = np.zeros((sample_n, allele_n), dtype='uint8')\n",
    "\n",
    "    # Generate centroids\n",
    "    centroids = []\n",
    "    for _ in range(n_clusters):\n",
    "        centroid = np.random.binomial(1, sparsity, size=allele_n)  # Sparse centroid\n",
    "        centroids.append(centroid)\n",
    "\n",
    "    # Assign samples to clusters and introduce variations\n",
    "    samples_per_cluster = sample_n // n_clusters\n",
    "    for i in range(n_clusters):\n",
    "        start_index = i * samples_per_cluster\n",
    "        end_index = min((i + 1) * samples_per_cluster, sample_n) \n",
    "\n",
    "        for j in range(start_index, end_index):\n",
    "            # Introduce variations based on the centroid\n",
    "            new_sample = centroids[i].copy()\n",
    "            flip_indices = np.random.choice(allele_n, size=int(cluster_spread * allele_n), replace=False)\n",
    "            new_sample[flip_indices] = 1 - new_sample[flip_indices]  # Flip bits\n",
    "            npy_matrix[j] = new_sample\n",
    "\n",
    "\n",
    "\n",
    "    guids = [f\"sample_{i}\" for i in np.arange(sample_n)]\n",
    "    alleles = [f\"allele_{i}\" for i in np.arange(allele_n)]\n",
    "    headers_json = {\"guids\": guids, \"alleles\": alleles}\n",
    "\n",
    "    return npy_matrix, headers_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/amorris/BioInf/Ardal_WD/databases/usher_barcodes.csv\", index_col=0)\n",
    "matrix_array = df.values.astype(np.uint8)\n",
    "\n",
    "np.save(f\"usher_matrix.npy\", matrix_array)\n",
    "headers = {\n",
    "            \"index\": df.index.tolist(),\n",
    "            \"columns\": df.columns.tolist()\n",
    "        }\n",
    "\n",
    "with open(f\"usher_headers.json\", \"w\") as f:\n",
    "    json.dump(headers, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = [\"./data/Cparv_headers.json\", \"./data/Cparv_matrix.npy\"]\n",
    "# bg_ard = Ardal(file_path)\n",
    "\n",
    "npy_matrix = np.ascontiguousarray(np.load(file_path[1]))\n",
    "with open(file_path[0], 'r') as fin:\n",
    "    json_headers = json.load(fin)\n",
    "\n",
    "print(len(json_headers[\"alleles\"]))\n",
    "\n",
    "allele_ids = [\"allele2\", \"allele7\", \"allele12\"]\n",
    "guids = [\"guid1\", \"guid5\", \"guid6\", \"guid10\"]\n",
    "coords = getCoords(json_headers, allele_ids=allele_ids, guids=guids)\n",
    "\n",
    "aid_guid_pairs = [decodeCoord(json_headers, coord) for coord in coords]\n",
    "\n",
    "bit_array = ardal.allele_set_membership(npy_matrix, coords)\n",
    "\n",
    "result = [aid_guid_pairs[i] for i, b in enumerate(bit_array) if b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_matrix, json_headers = generateMatrix(sample_n=100000, allele_n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guids = list(np.random.choice(json_headers[\"guids\"], 5))\n",
    "allele_ids = list(np.random.choice(json_headers[\"alleles\"], 5))\n",
    "\n",
    "coords = getCoords(json_headers, allele_ids=allele_ids, guids=guids)\n",
    "aid_guid_pairs = [decodeCoord(json_headers, coord) for coord in coords]\n",
    "\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_matrix = np.load(\"./data/test_csv_matrix.npy\")\n",
    "# test_matrix = np.array([[0, 1, 1, 0],[1, 0, 0, 1], [0, 1, 0, 1], [1, 0, 0, 1]])\n",
    "# coords = np.array([[0, 0], [0, 1], [0, 2], [0, 3], [2, 1]], dtype=int)\n",
    "coords = randcoords(npy_matrix, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_and_save(allele_matrix, output_prefix):\n",
    "    \"\"\"Isolates binary matrix, converts to uint8, and saves array and headers.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Convert directly to uint8 NumPy array\n",
    "        matrix_array = allele_matrix.values.astype(np.uint8)\n",
    "\n",
    "        # Save NumPy array\n",
    "        np.save(f\"{output_prefix}_matrix.npy\", matrix_array)\n",
    "\n",
    "        # Store headers\n",
    "        headers = {\n",
    "            \"guids\": allele_matrix.index.tolist(),\n",
    "            \"alleles\": allele_matrix.columns.tolist()\n",
    "        }\n",
    "\n",
    "        with open(f\"{output_prefix}_headers.json\", \"w\") as f:\n",
    "            json.dump(headers, f, indent=4)\n",
    "\n",
    "        print(f\"Matrix saved as '{output_prefix}_matrix.npy', headers as '{output_prefix}_headers.json'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "df = pd.read_csv(\"./data/Cparv_matrix.csv\", header=0, index_col=0)\n",
    "isolate_and_save(df, \"Cparv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "file_path = [\"./data/test_csv_headers.json\", \"./data/test_csv_matrix.npy\"]\n",
    "# bg_ard = Ardal(file_path)\n",
    "\n",
    "npy_matrix = np.load(file_path[1])\n",
    "with open(file_path[0], 'r') as fin:\n",
    "    json_headers = json.load(fin)\n",
    "\n",
    "allele_ids = [\"allele2\", \"allele7\", \"allele12\"]\n",
    "coords = getCoords(json_headers, allele_ids=allele_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(npy_matrix)\n",
    "print(coords)\n",
    "s = time.time()\n",
    "print(np.array([npy_matrix[row, col] for row, col in coords]))\n",
    "e = time.time()\n",
    "print(s-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy_matrix, json_headers = generateMatrix(sample_n=10000, allele_n=500)\n",
    "npy_matrix = np.ascontiguousarray(np.load(\"./data/BG_pan_matrix.npy\"))\n",
    "print(len(npy_matrix[0])*len(npy_matrix))\n",
    "\n",
    "coords = randcoords(npy_matrix, 10)\n",
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import _ardal\n",
    "\n",
    "npy_matrix = np.ascontiguousarray(np.load(\"../data/usher_matrix.npy\"))\n",
    "with open(\"../data/usher_headers.json\", 'r') as fin:\n",
    "    json_headers = json.load(fin)\n",
    "\n",
    "print(npy_matrix.size * npy_matrix.itemsize)\n",
    "\n",
    "# s = time.time()\n",
    "ardmat = _ardal.AlleleMatrix(npy_matrix)\n",
    "# result = ardmat.hamming()\n",
    "# result = ardal_mat.access(coords)\n",
    "# result = ardal.accessAlleleMatrix(npy_matrix, coords)\n",
    "# e = time.time()\n",
    "# print(npy_matrix.dtype, result)\n",
    "# print(e-s)\n",
    "\n",
    "## uint8 provides a massive speedup compared to int64 or int8\n",
    "# packed = npy_matrix.astype('int8')\n",
    "# print(packed.size * packed.itemsize)\n",
    "\n",
    "# s = time.time()\n",
    "# result = ardal.accessAlleleMatrix(packed, coords)\n",
    "# e = time.time()\n",
    "# print(packed.dtype, result)\n",
    "# print(e-s)\n",
    "\n",
    "neigh = ardmat.neighbourhoodSIMD(41, 2)\n",
    "neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "ard_d_out = np.array(squareform(result))\n",
    "ard_d_df = pd.DataFrame(ard_d_out, columns=json_headers[\"guids\"], index=json_headers[\"guids\"])\n",
    "ard_d_df.to_csv(\"usher_hamming.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 19999900000.0\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 74.5 GiB for an array with shape (19999900000,) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(_n, _n\u001b[38;5;241m*\u001b[39m(_n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m ardmat \u001b[38;5;241m=\u001b[39m _ardal\u001b[38;5;241m.\u001b[39mAlleleMatrix(npy_matrix)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mardmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhamming\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 74.5 GiB for an array with shape (19999900000,) and data type int32"
     ]
    }
   ],
   "source": [
    "import _ardal\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import requests\n",
    "\n",
    "\n",
    "# npy_matrix = np.ascontiguousarray(np.load(\"./data/usher_matrix.npy\"))\n",
    "# with open(\"./data/usher_headers.json\", 'r') as fin:\n",
    "#     json_headers = json.load(fin)\n",
    "# npy_matrix, json_headers = generateClusteredSparseMatrix(sample_n=200000, allele_n=5000, sparsity=0.01, cluster_spread=0.0001, n_clusters=100)\n",
    "# npy_matrix = np.ascontiguousarray(\\\n",
    "#     [[1, 1, 1, 1, 1], \n",
    "#      [0, 1, 1, 1, 1], \n",
    "#      [0, 0, 1, 1, 1], \n",
    "#      [0, 0, 0, 1, 1], \n",
    "#      [0, 0, 0, 0, 1], \n",
    "#      [0, 0, 0, 0, 0], \n",
    "#      [0, 1, 0, 1, 0], \n",
    "#      [1, 0, 1, 0, 1]])\n",
    "\n",
    "\n",
    "_n = len(npy_matrix)\n",
    "print(_n, _n*(_n-1)/2)\n",
    "\n",
    "ardmat = _ardal.AlleleMatrix(npy_matrix)\n",
    "ardmat.hamming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ardal import Ardal\n",
    "\n",
    "data = [\"/home/amorris/BioInf/Ardal/data/Cparv_matrix.npy\", \"/home/amorris/BioInf/Ardal/data/Cparv_headers.json\"]\n",
    "ard = Ardal(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard.neighbourhood(\"SRR6147472_UKP3\", 100, simd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "guid = \"XCH.1\"\n",
    "e_guid = encodeGuid(json_headers, guid)\n",
    "\n",
    "s = time.time()\n",
    "simd_result = ardmat.neighbourhoodSIMD(e_guid, n)\n",
    "e = time.time()\n",
    "simd_dr = sorted([[decodeGuid(json_headers, r), c] for r, c in simd_result], key=lambda x: x[0])\n",
    "print(e-s, len(simd_dr))\n",
    "\n",
    "# s = time.time()\n",
    "# result = ardmat.neighbourhood(e_guid, n)\n",
    "# e = time.time()\n",
    "# print(e-s, [decodeGuid(json_headers, r) for r in result])\n",
    "\n",
    "# s = time.time()\n",
    "# cw_result = requests.get(f\"http://localhost:5000/neighbours/{guid}/{n+10}\").json()\n",
    "# e = time.time()\n",
    "# print(e-s, sorted([[i, int(j)] for i, j in cw_result if int(j) <= n], key=lambda x: x[0]))\n",
    "\n",
    "# ard_d_df = pd.DataFrame(np.array(squareform(result)), columns=json_headers[\"guids\"], index=json_headers[\"guids\"])\n",
    "# ard_d_df.to_csv(\"./clustered_sparse_sim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "s = time.time()\n",
    "cw_result = requests.get(f\"http://localhost:5000/neighbours/{guid}/{n-1}\").json()\n",
    "e = time.time()\n",
    "\n",
    "print(e-s)\n",
    "\n",
    "cw_dr = set(sorted([i for i, c in cw_result if int(c)<=n-1]))\n",
    "\n",
    "intersection = cw_dr.intersection(simd_dr)\n",
    "union = cw_dr.union(simd_dr)\n",
    "symmetric_difference = cw_dr.symmetric_difference(simd_dr)\n",
    "\n",
    "# print(f\"Intersection: {intersection}\")\n",
    "# print(f\"Union: {union}\")\n",
    "# print(f\"Symmetric Difference: {symmetric_difference}\")\n",
    "\n",
    "# print(simd_dr)\n",
    "print([(i, c) for i, c in cw_result if int(c)<=n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, c in cw_result:\n",
    "#     ard_d = ard_d_df[guid][i]\n",
    "#     if int(ard_d) != int(c):\n",
    "#         print(i, c, ard_d)\n",
    "\n",
    "ard_d_df[guid]['A.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ardmat = ardal.AlleleMatrix(npy_matrix)\n",
    "\n",
    "s = time.time()\n",
    "# result = ardmat.hamming()\n",
    "result = ardmat.neighbourhood(6969, 175)\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print(e-s, len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
