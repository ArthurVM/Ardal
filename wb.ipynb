{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "# %matplotlib qt\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from ardal import Ardal\n",
    "import _ardal\n",
    "\n",
    "\n",
    "def makePCA(matrix, categories, D3=True, Dplus=False):\n",
    "    \"\"\"\n",
    "    Performs PCA on a matrix and visualizes the results with a legend.\n",
    "\n",
    "    Args:\n",
    "        matrix (pd.DataFrame): The input matrix (rows are samples, columns are features).\n",
    "        categories (dict): A dictionary mapping matrix indices (sample IDs) to categories.\n",
    "        D3 (bool): Whether to create a 3D plot (default: True).\n",
    "        Dplus (bool): Whether to use a size map (default: False).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the matrix is not a Pandas DataFrame or if categories is not a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    if not isinstance(matrix, pd.DataFrame):\n",
    "        raise ValueError(\"matrix must be a Pandas DataFrame.\")\n",
    "    if not isinstance(categories, dict):\n",
    "        raise ValueError(\"categories must be a dictionary.\")\n",
    "\n",
    "    # Color setup\n",
    "    unique_categories = sorted(set(categories.values()))\n",
    "    cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "    colours = [cmap(i) for i in np.linspace(0, 1, len(unique_categories))]\n",
    "    cdict = dict(zip(unique_categories, colours))\n",
    "\n",
    "    matrix_indices = matrix.index\n",
    "    matrix_standardized = StandardScaler().fit_transform(matrix)\n",
    "\n",
    "    n = min(len(matrix_indices), 20)\n",
    "    pca = PCA(n_components=n)\n",
    "    pca_result = pca.fit_transform(matrix_standardized)\n",
    "    pca_df = pd.DataFrame(data=pca_result[:, :4], columns=['PC1', 'PC2', 'PC3', 'PC4'], index=matrix_indices)\n",
    "\n",
    "    # Plotting\n",
    "    if D3:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        if Dplus:\n",
    "            min_val = pca_df[\"PC4\"].min()\n",
    "            max_val = pca_df[\"PC4\"].max()\n",
    "            sizemap = 100 + ((pca_df['PC4'] - min_val) * 100) / (max_val - min_val)\n",
    "            scatter = ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'], s=sizemap, alpha=0.8)\n",
    "        else:\n",
    "            for category in unique_categories:\n",
    "                indices = [idx for idx, cat in categories.items() if cat == category]\n",
    "                subset = pca_df.loc[indices]\n",
    "                scatter = ax.scatter(subset['PC1'], subset['PC2'], subset['PC3'], alpha=0.8, c=[cdict[category]] * len(subset), label=category)\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "        ax.set_zlabel('PC3')\n",
    "\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        if Dplus:\n",
    "            min_val = pca_df['PC3'].min()\n",
    "            max_val = pca_df['PC3'].max()\n",
    "            sizemap = 200 + ((pca_df['PC3'] - min_val) * 50) / (max_val - min_val)\n",
    "            scatter = ax.scatter(pca_df['PC1'], pca_df['PC2'], s=sizemap, alpha=0.7)\n",
    "        else:\n",
    "            for category in unique_categories:\n",
    "                indices = [idx for idx, cat in categories.items() if cat == category]\n",
    "                subset = pca_df.loc[indices]\n",
    "                scatter = ax.scatter(subset['PC1'], subset['PC2'], alpha=0.8, c=[cdict[category]] * len(subset), label=category)\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def cluster(dist_matrix, nclusters):\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    dist_array = dist_matrix.values\n",
    "\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=nclusters, random_state=0, n_init='auto')  # Set random_state for reproducibility\n",
    "    cluster_labels = kmeans.fit_predict(dist_array)\n",
    "\n",
    "    # Create the cluster membership dictionary\n",
    "    cluster_membership = {}\n",
    "    for i, guid in enumerate(dist_matrix.index):\n",
    "        cluster_membership[guid] = cluster_labels[i]\n",
    "\n",
    "    return cluster_membership\n",
    "    \n",
    "\n",
    "\n",
    "# data = [\"./data/Cparv_matrix.npy\", \"./data/Cparv_headers.json\"]\n",
    "# ard = Ardal(data)\n",
    "# ard.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def create_test_dataset():\n",
    "    \"\"\"\n",
    "    Creates a simulated test dataset for testing the Ardal.unique functionality.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the matrix (NumPy array) and headers (dictionary).\n",
    "\n",
    "    \"SNP1\" is unique to \"GUID1\" and \"GUID2\".\n",
    "    \"SNP2\" is unique to \"GUID3\", \"GUID4\", and \"GUID5\".\n",
    "    \"SNP3\" is unique to \"GUID6\" and \"GUID7\".\n",
    "    \"SNP4\" is unique to \"GUID8\", \"GUID9\", and \"GUID10\".\n",
    "    \"SNP5\" is present in all GUIDs.\n",
    "    \"SNP6\" is present in GUID1, GUID2, GUID3, GUID4, GUID5, GUID6, GUID7.\n",
    "    \"SNP7\" is present in GUID1, GUID2, GUID3, GUID4, GUID5, GUID6, GUID7, GUID8, GUID9, GUID10.\n",
    "    \"SNP8\" is present in GUID1, GUID2, GUID3, GUID4, GUID5.\n",
    "    \"SNP9\" is present in GUID6, GUID7, GUID8, GUID9, GUID10.\n",
    "    \"SNP10\" is present in GUID1, GUID2, GUID3, GUID4, GUID5, GUID6, GUID7, GUID8, GUID9.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define GUIDs and SNPs\n",
    "    guids = [f\"GUID{i}\" for i in range(1, 11)]  # GUID1 to GUID10\n",
    "    alleles = [f\"SNP{i}\" for i in range(1, 11)]  # SNP1 to SNP10\n",
    "\n",
    "    # Create an empty matrix (all zeros)\n",
    "    matrix = np.zeros((len(guids), len(alleles)), dtype=np.uint8)\n",
    "\n",
    "    # Set unique SNPs\n",
    "    matrix[0, 0] = 1  # SNP1 in GUID1\n",
    "    matrix[1, 0] = 1  # SNP1 in GUID2\n",
    "    matrix[2, 1] = 1  # SNP2 in GUID3\n",
    "    matrix[3, 1] = 1  # SNP2 in GUID4\n",
    "    matrix[4, 1] = 1  # SNP2 in GUID5\n",
    "    matrix[5, 2] = 1  # SNP3 in GUID6\n",
    "    matrix[6, 2] = 1  # SNP3 in GUID7\n",
    "    matrix[7, 3] = 1  # SNP4 in GUID8\n",
    "    matrix[8, 3] = 1  # SNP4 in GUID9\n",
    "    matrix[9, 3] = 1  # SNP4 in GUID10\n",
    "\n",
    "    # Set SNP5 to be present in all GUIDs\n",
    "    matrix[:, 4] = 1  # SNP5 in all GUIDs\n",
    "\n",
    "    # Set SNP6 to be present in GUID1, GUID2, GUID3, GUID4, GUID5, GUID6, GUID7\n",
    "    matrix[0:7, 5] = 1\n",
    "\n",
    "    # Set SNP7 to be present in all GUIDs\n",
    "    matrix[:, 6] = 1\n",
    "\n",
    "    # Set SNP8 to be present in GUID1, GUID2, GUID3, GUID4, GUID5\n",
    "    matrix[0:5, 7] = 1\n",
    "\n",
    "    # Set SNP9 to be present in GUID6, GUID7, GUID8, GUID9, GUID10\n",
    "    matrix[5:10, 8] = 1\n",
    "\n",
    "    # Set SNP10 to be present in GUID1, GUID2, GUID3, GUID4, GUID5, GUID6, GUID7, GUID8, GUID9\n",
    "    matrix[0:9, 9] = 1\n",
    "\n",
    "    # Create headers\n",
    "    headers = {\n",
    "        \"guids\": guids,\n",
    "        \"alleles\": alleles\n",
    "    }\n",
    "\n",
    "    return matrix, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard.neighbourhood(\"SRR6147472_UKP3\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "b20_n = ard.neighbourhood(\"UKP215\", 200)\n",
    "e = time.time()\n",
    "\n",
    "h_D = ard.pairwise(metric=\"hamming\")\n",
    "\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(zip(ard._headers[\"guids\"], cluster_assignments))\n",
    "makePCA(h_D, cluster_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPP package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _ardal\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# d = 100000\n",
    "# g = \"SPT12274\"\n",
    "\n",
    "# matrix = np.ascontiguousarray(np.load(\"/home/amorris/BioInf/SSD/BioInf/Plasmodium_vcfs/Pf_SPT_matrix.npy\"))\n",
    "# with open(\"/home/amorris/BioInf/SSD/BioInf/Plasmodium_vcfs/Pf_SPT_headers.json\", \"r\") as f:\n",
    "#     headers = json.load(f)\n",
    "\n",
    "# cpp_ard = _ardal.AlleleMatrix(matrix)\n",
    "\n",
    "def encodeGuid( guid : str ):\n",
    "    return headers[\"guids\"].index(guid)\n",
    "\n",
    "def decodeGuid( row_coord : int ):\n",
    "    return headers[\"guids\"][row_coord]\n",
    "\n",
    "def encodeAllele( allele : str ):\n",
    "    return headers[\"alleles\"].index(allele)\n",
    "\n",
    "def decodeAllele( col_coord : int ):\n",
    "    return headers[\"alleles\"][col_coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid_i = encodeGuid(g)\n",
    "\n",
    "cpp_ard.neighbourhoodSIMD(guid_i, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ardal import Ardal\n",
    "import pandas as pd\n",
    "\n",
    "# data = [\"/home/amorris/BioInf/SSD/BioInf/Plasmodium_vcfs/Pf_matrix.npy\", \"/home/amorris/BioInf/SSD/BioInf/Plasmodium_vcfs/Pf_headers.json\"]\n",
    "data = [\"/home/amorris/BioInf/burkholderia_WD/data/allele_matrices/BG_core_matrix.npy\", \"/home/amorris/BioInf/burkholderia_WD/data/allele_matrices/BG_core_headers.json\"]\n",
    "ard = Ardal(data)\n",
    "ard.stats()\n",
    "# df = pd.read_csv(\"/home/amorris/BioInf/ProtoDB/WD/Pf_SPT_hamming.csv\",  index_col=0)\n",
    "meta = pd.read_csv(\"/home/amorris/BioInf/burkholderia_WD/data/Mullins_Onion_Jones.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GUID6', 'GUID9', 'GUID2', 'GUID8', 'GUID1', 'GUID3', 'GUID5', 'GUID7', 'GUID4']\n"
     ]
    }
   ],
   "source": [
    "matrix, headers = create_test_dataset()\n",
    "ard = Ardal([matrix, headers])\n",
    "cpp_ard = _ardal.AlleleMatrix(matrix)\n",
    "\n",
    "alleles = [\"SNP5\", \"SNP10\"]\n",
    "n = len(alleles)\n",
    "allele_coords = [encodeAllele(allele) for allele in alleles]\n",
    "input_coords = np.array([[encodeGuid(guid), allele] for guid in ard.getHeaders()[\"guids\"] for allele in allele_coords])\n",
    "access_result = zip(input_coords, cpp_ard.access(input_coords))\n",
    "# print(list(result))\n",
    "decoded_results = [decodeGuid(guid_c) for (guid_c, allele_c), r in access_result if r==1]\n",
    "\n",
    "results = [guid for guid in set(decoded_results) if decoded_results.count(guid) == n]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "d = 90000\n",
    "g = \"ERR1100643\"\n",
    "# ard.flushCache()\n",
    "s = time.time()\n",
    "ard_neigh = ard.neighbourhood(g, d, simd=True)\n",
    "e = time.time()\n",
    "\n",
    "ard_neigh\n",
    "e-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = dict(zip(list(meta[\"SRA\"]), list(meta[\"Clade\"])))\n",
    "clus = cluster(ardD_hamming, 10)\n",
    "makePCA(ardD_hamming, clus, D3=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid_ids = [guid for guid, c in clus.items() if c in [5]]\n",
    "print(guid_ids)\n",
    "d1 = ard.unique(guid_ids)\n",
    "\n",
    "d = {'GCA_902830895.1_ASM90283089v1_genomic.4394500.T',\n",
    " 'GCA_902830905.1_ASM90283090v1_genomic.1733623.C',\n",
    " 'GCA_902830905.1_ASM90283090v1_genomic.1759564.C',\n",
    " 'GCA_902830905.1_ASM90283090v1_genomic.3025340.A',\n",
    " 'GCA_902830905.1_ASM90283090v1_genomic.4319502.C',\n",
    " 'GCA_902831285.1_ASM90283128v1_genomic.4462753.T',\n",
    " 'GCA_902831285.1_ASM90283128v1_genomic.446892.C'}\n",
    "\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2000000\n",
    "g = \"SPT12274\"\n",
    "neigh = df.loc[g][df.loc[g] < d].to_dict()\n",
    "ard_neigh = ard.neighbourhood(g, d, simd=True)\n",
    "\n",
    "results_box = {\"ardal\" : ard_neigh, \"hamming\" : neigh}\n",
    "\n",
    "results_df = pd.DataFrame(results_box).sort_values(by=\"ardal\", ascending=True)\n",
    "results_df = results_df.drop(g)\n",
    "\n",
    "matching_guids = []\n",
    "mismatching_guids = []\n",
    "\n",
    "for guid in results_df.index:\n",
    "    if results_df.loc[guid, \"ardal\"] == results_df.loc[guid, \"hamming\"]:\n",
    "        matching_guids.append(guid)\n",
    "    else:\n",
    "        mismatching_guids.append(guid)\n",
    "\n",
    "# print(bools)\n",
    "print(f\"MATCH : {len(matching_guids)}\")\n",
    "print(f\"MISMATCH : {len(mismatching_guids)} {mismatching_guids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[\"SPT43391\"]\n",
    "len(headers[\"alleles\"])%32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_samples = 100\n",
    "# num_alleles = 50\n",
    "# allele_matrix = np.random.randint(0, 2, (num_samples, num_alleles))\n",
    "\n",
    "K = 4\n",
    "\n",
    "X=ard.getMatrix()\n",
    "nmf = NMF(n_components=K, init='random', random_state=12)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "W = W/np.sum(W, axis=1)[:, np.newaxis]\n",
    "\n",
    "cluster_assignments = np.argmax(H, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bottom = np.zeros(ard.stats()[\"n_guids\"])\n",
    "for k in range(K):\n",
    "    ax.bar(range(ard.stats()[\"n_guids\"]), W[:, k], bottom=bottom, label=f'Ancestry {k+1}')\n",
    "    bottom += W[:, k]\n",
    "\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Admixture Proportion')\n",
    "ax.set_title(f'Admixture Proportions (K={K})')\n",
    "ax.legend()\n",
    "\n",
    "guids = ard.getHeaders()[\"guids\"]\n",
    "plt.xticks(range(len(guids)), guids, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "\n",
    "# 1. Dimensionality Reduction (if necessary)\n",
    "if W.shape[1] > 3:\n",
    "    pca = PCA(n_components=3)  # Reduce to 3D for plotting\n",
    "    W_reduced = pca.fit_transform(W)\n",
    "elif W.shape[1] == 3:  # Already 3D\n",
    "    W_reduced = W\n",
    "else: # If W has fewer than 3 components, we'll pad it with zeros.\n",
    "    W_reduced = np.pad(W, ((0,0), (0, 3 - W.shape[1])), mode='constant')\n",
    "\n",
    "\n",
    "\n",
    "# 2. 3D Scatter Plot\n",
    "# col_cat = np.argmax(W, axis=1)  # Get dominant ancestry for coloring\n",
    "col_cat = np.max(W, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d') # Create a 3D axes\n",
    "\n",
    "\n",
    "scatter = ax.scatter(W_reduced[:, 0], W_reduced[:, 1], W_reduced[:, 2], c=col_cat, cmap='viridis')\n",
    "\n",
    "\n",
    "ax.set_title(\"NMF Admixture 3D Scatter Plot\")\n",
    "\n",
    "\n",
    "# Set labels based on dimensionality reduction or original W matrix:\n",
    "if W.shape[1] > 3:  # If PCA was used\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.set_zlabel(\"Principal Component 3\")\n",
    "elif W.shape[1] == 3:\n",
    "    ax.set_xlabel(\"Ancestry Component 1\")  # Adjust if you have specific names\n",
    "    ax.set_ylabel(\"Ancestry Component 2\")\n",
    "    ax.set_zlabel(\"Ancestry Component 3\")\n",
    "else:\n",
    "    ax.set_xlabel(\"Ancestry Component 1\")\n",
    "    ax.set_ylabel(\"Ancestry Component 2\")\n",
    "    ax.set_zlabel(\"Padding\") # Indicate that the third dimension is padding\n",
    "\n",
    "\n",
    "fig.colorbar(scatter, label=\"Dominant Ancestry\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'green', 'blue', 'purple']\n",
    "\n",
    "# Number of samples and ancestries\n",
    "n_samples, n_ancestries = W.shape\n",
    "\n",
    "# Create the admixture bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust figure size if needed\n",
    "\n",
    "bottom = np.zeros(n_samples)  # Starting point for each bar segment\n",
    "\n",
    "for k in range(n_ancestries):\n",
    "    ax.bar(range(n_samples), W[:, k], bottom=bottom, color=colors[k % len(colors)], label=f\"Ancestry {k+1}\")\n",
    "    bottom += W[:, k]  # Update the bottom for the next ancestry component\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(range(n_samples))\n",
    "ax.set_xticklabels(range(1, n_samples + 1))  # Or use sample names if available\n",
    "ax.set_xlabel(\"Sample\")\n",
    "ax.set_ylabel(\"Admixture Proportion\")\n",
    "ax.set_title(\"Admixture Bar Plot\")\n",
    "\n",
    "ax.legend(title=\"Ancestries\")\n",
    "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Number of samples and ancestries\n",
    "n_samples, n_ancestries = W.shape\n",
    "\n",
    "# Create the admixture bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust figure size if needed\n",
    "\n",
    "bottom = np.zeros(n_samples)  # Starting point for each bar segment\n",
    "\n",
    "for k in range(n_ancestries):\n",
    "    ax.bar(range(n_samples), W[:, k], bottom=bottom, color=colors[k % len(colors)], label=f\"Ancestry {k+1}\")\n",
    "    bottom += W[:, k]  # Update the bottom for the next ancestry component\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(range(n_samples))\n",
    "ax.set_xticklabels(range(1, n_samples + 1))  # Or use sample names if available\n",
    "ax.set_xlabel(\"Sample\")\n",
    "ax.set_ylabel(\"Admixture Proportion\")\n",
    "ax.set_title(\"Admixture Bar Plot\")\n",
    "\n",
    "ax.legend(title=\"Ancestries\")\n",
    "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(W, axis=1))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guids = list(b20_n.keys())\n",
    "print(guids)\n",
    "s = time.time()\n",
    "core = ard.core(guids, 0.9)\n",
    "e = time.time()\n",
    "\n",
    "accessory = ard.accessory(guids, 0.9)\n",
    "\n",
    "print(e-s)\n",
    "print(len(core), len(accessory), len(core)+len(accessory))\n",
    "\n",
    "d = defaultdict(list)\n",
    "for c in core:\n",
    "    d[c.split(\".\")[0]].append(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
